{"bert_config_file": "/home/sakus/ws/pytorch-pretrained-BERT/data/uncased_L-24_H-1024_A-16/bert_config.json", "vocab_file": "/home/sakus/ws/pytorch-pretrained-BERT/data/uncased_L-24_H-1024_A-16/vocab.txt", "output_dir": "coqa_output/train_doc_shuff_72h_fixed", "task_name": "coqa", "dataset_option": null, "input_ablation": "shuffle_document_words", "cache_dir": "cache", "corenlp_cache_dir": "corenlp_{}", "entity_anonymization": null, "data_dir": "/uge_mnt/home/sakus/data/coqa", "init_checkpoint": "/home/sakus/ws/pytorch-pretrained-BERT/data/uncased_L-24_H-1024_A-16/pytorch_model.bin", "do_lower_case": true, "max_seq_length": 384, "doc_stride": 128, "max_query_length": 64, "do_train": true, "do_eval": true, "do_test": false, "train_batch_size": 24, "eval_batch_size": 64, "learning_rate": 2.5e-05, "num_train_epochs": 2.0, "warmup_proportion": 0.1, "save_checkpoints_steps": 200, "loss_report_steps": 0, "eval_steps": 200, "iterations_per_loop": 1000, "n_best_size": 20, "max_answer_length": 30, "verbose_logging": false, "no_cuda": false, "seed": 42, "gradient_accumulation_steps": 1, "local_rank": -1, "optimize_on_cpu": false, "fp16": true, "loss_scale": 128.0, "log_spec": null, "no_cache": true, "eval_on_train": false, "allow_impossible": false, "null_score_diff_threshold": 0.0, "ignore_out_of_span": false, "output_statistics": false}